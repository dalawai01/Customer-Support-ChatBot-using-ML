{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q5ElLO0bK8m"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gxkkk4Ady1S",
        "outputId": "d9d86464-f44a-4fa9-ed60-3fb0f693ae08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'score': 0.10617639869451523, 'token': 2210, 'token_str': 'little', 'sequence': 'the quick brown fox jumps over the little dog.'}, {'score': 0.06955333799123764, 'token': 2502, 'token_str': 'big', 'sequence': 'the quick brown fox jumps over the big dog.'}, {'score': 0.04759746417403221, 'token': 2235, 'token_str': 'small', 'sequence': 'the quick brown fox jumps over the small dog.'}, {'score': 0.02374725602567196, 'token': 9696, 'token_str': 'startled', 'sequence': 'the quick brown fox jumps over the startled dog.'}, {'score': 0.02024279721081257, 'token': 5777, 'token_str': 'sleeping', 'sequence': 'the quick brown fox jumps over the sleeping dog.'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for masked language modeling\n",
        "nlp_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test the pipeline with a simple input\n",
        "test_sentence = \"The quick brown fox jumps over the [MASK] dog.\"\n",
        "result = nlp_pipeline(test_sentence)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "7PYsMm1TeReJ",
        "outputId": "250ab2a3-82dc-414a-f869-6549e82b5123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-__4llv_2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-__4llv_2\n",
            "  Resolved https://github.com/huggingface/transformers to commit fee86516a48c92133847fc7b44ca2f83c7c5634d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9735169 sha256=3b6e51e6c00e238aecd776a8d252c744f195fbbe81e98a37c217e259a37fe687\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x4z0up52/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed transformers-4.45.0.dev0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0a8b8f10fe934f75b6e9b165ef120079",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TLcqETbfSS1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr-x9kV1f880"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgxTgxFsgMX9",
        "outputId": "3abf6303-fdbf-4e7d-8c2c-ec55e05ff4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "# Replace 'your-access-token' with your actual Hugging Face access token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c9230f4b77fa4e8e840b1b0d010e1331",
            "bd0deb3861a34853ac6dee48c977190b",
            "dbe99ed017b64b09a59612916a838e0c",
            "55cc544138e04bb891ca23aa46a24f33",
            "82f1f3675e404a17a7ca3421f8098b4e",
            "337235a39fcb47a59032d42fb0735dd5",
            "12dd966286594e1f9b250a023d326adc",
            "6a1585cde5e440b78498782c1d470bf9",
            "64eb7255a44f4fb28c9da034807678b8",
            "1d95e084c3a44967acd127821d7164fa",
            "7357b5c1349240218bd04a3d9e99cb48"
          ]
        },
        "id": "JF8iNf73gnT8",
        "outputId": "80b8ec48-15d7-4b63-ef4d-63c9f4cb2136"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9230f4b77fa4e8e840b1b0d010e1331"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfX6Ycbvgsl-",
        "outputId": "0e8273a8-a0f0-4b6b-a91d-415c0a4c1db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.45.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMq1bw6hXNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9478809c-a180-4cfb-b45f-ac8b12792dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KhW1Qgyhf_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fe51b8-0112-486d-93fd-1069384a8fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample_prompt = \"Hello, how are you?\"\n",
        "\n",
        "\n",
        "input_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2)\n",
        "\n",
        "\n",
        "#Decode the output back to text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "#Finally, output the respnose\n",
        "print(f\"Generated Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF0ifwVuhuE1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create an NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test the NER pipeline with a sample sentence\n",
        "test_sentence = \"Apple is planning to build a new campus in Austin.\"\n",
        "result = ner_pipeline(test_sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWd8C-Aui-Jv"
      },
      "outputs": [],
      "source": [
        "def display_masked_sentence(sentence, ner_results):\n",
        "    masked_sentence = sentence\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        entity_label = entity['entity']\n",
        "        masked_sentence = masked_sentence.replace(entity_word, f\"[{entity_label}]\")\n",
        "    return masked_sentence\n",
        "\n",
        "# Test the function with the NER results\n",
        "masked_sentence = display_masked_sentence(test_sentence, result)\n",
        "print(masked_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVqJ8xDrjRW_"
      },
      "outputs": [],
      "source": [
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Test the summarizer with a longer input text\n",
        "long_text = (\n",
        "    \"The quick brown fox jumps over the lazy dog. The lazy dog, however, was not really lazy. \"\n",
        "    \"It was simply tired from chasing after the quick brown fox all day. The two animals had a \"\n",
        "    \"long history of playful rivalry, with the fox always outwitting the dog. Despite their differences, \"\n",
        "    \"they shared a bond of mutual respect and friendship.\"\n",
        ")\n",
        "\n",
        "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgP7AiKDjmu3"
      },
      "outputs": [],
      "source": [
        "# Load the QA pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Define the context and the question\n",
        "context = (\n",
        "    \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California, \"\n",
        "    \"that designs, develops, and sells consumer electronics, computer software, and online services. \"\n",
        "    \"It is considered one of the Big Five companies in the U.S. information technology industry, along with \"\n",
        "    \"Amazon, Google, Microsoft, and Facebook.\"\n",
        ")\n",
        "question = \"Where is Apple Inc. headquartered?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee1NWILijvsl"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtqR9ELulCZh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSnpA_jGlGW4"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Authentication\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHkXdhjblVWz"
      },
      "outputs": [],
      "source": [
        "# Define the path for the CSV file\n",
        "csv_file = 'qa_dataset.csv'\n",
        "\n",
        "# Check if the CSV file exists; if not, create it with initial data\n",
        "if not os.path.exists(csv_file):\n",
        "    qa_data = {\n",
        "        'question': [\"What is the name of Julius Magellan's dog?\", \"Who is Julius Magellan's dog?\"],\n",
        "        'answer': [\"The name of Julius Magellan's dog is Sparky\", \"Julius Magellan's dog is called Sparky\"]\n",
        "    }\n",
        "    qa_df = pd.DataFrame(qa_data)\n",
        "    qa_df.to_csv(csv_file, index=False)\n",
        "else:\n",
        "    # Load the existing CSV file into a DataFrame\n",
        "    qa_df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X17LYWPKldDU"
      },
      "outputs": [],
      "source": [
        "qa_df = pd.read_csv(csv_file)\n",
        "print(qa_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cmji7-xltRW"
      },
      "outputs": [],
      "source": [
        "# Initialize the Llama 2 model and tokenizer\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "# Initialize the pipeline using Hugging Face pipeline\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1024, ) # Adjust max_length as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BOg6HUMlwdC"
      },
      "outputs": [],
      "source": [
        "def answer_question(question):\n",
        "    global qa_df\n",
        "    # Check if the question is in the QA dataset\n",
        "    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n",
        "\n",
        "    if not answer.empty:\n",
        "        # Return the first matching answer\n",
        "        print(f\"Answer from QA dataset: {answer.iloc[0]}\")\n",
        "    else:\n",
        "        # Use Llama 2 to generate an answer\n",
        "        response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']\n",
        "\n",
        "        # Ensure the response doesn't redundantly include the question or incorrectly repeat \"Answer\"\n",
        "        response = response.replace(f\"Answer: {question}\", \"\").strip()\n",
        "        print(f\"Answer from Llama 2: {response}\")\n",
        "\n",
        "        # Add the new QA pair to the dataset if it's not already present\n",
        "        if not any(qa_df['question'].str.lower() == question.lower()):\n",
        "            new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n",
        "            qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
        "            qa_df.to_csv(csv_file, index=False)\n",
        "            print(\"New QA pair added to the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRj2RSisl6mG"
      },
      "outputs": [],
      "source": [
        "question_1 = \"What is the name of Julius Magellan's dog?\"\n",
        "answer_question(question_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxIfv6sVl9KX"
      },
      "outputs": [],
      "source": [
        "question_2 = \"Who is Julius Magellan's dog?\"\n",
        "answer_question(question_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlJsqzEbl_Ai"
      },
      "outputs": [],
      "source": [
        "# This should fallback to Llama 2 and then get added\n",
        "# to the dataset\n",
        "question_3 = \"What is the capital of France?\"\n",
        "answer_question(question_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIE5aBpmmBNZ"
      },
      "outputs": [],
      "source": [
        "print(qa_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b32aMO0CmDDX"
      },
      "outputs": [],
      "source": [
        "!pip -q install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWIS2JMDmFMn"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_chat_interface(question):\n",
        "    global qa_df\n",
        "    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n",
        "\n",
        "    if not answer.empty:\n",
        "        return f\"Answer from QA dataset: {answer.iloc[0]}\"\n",
        "    else:\n",
        "        response = llama_pipeline(question, max_length=150, do_sample=True)[0]['generated_text']\n",
        "        response = response.replace(f\"Answer: {question}\", \"\").strip()\n",
        "        # Add new question-answer pair to the dataset\n",
        "        if not any(qa_df['question'].str.lower() == question.lower()):\n",
        "            new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n",
        "            qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
        "            qa_df.to_csv(csv_file, index=False)\n",
        "            return f\"Answer from Llama 2: {response} \\n(New QA pair added to the dataset.)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQScJUPympeP"
      },
      "outputs": [],
      "source": [
        "# Create a Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_chat_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Customer Support Chatbot using ML\",\n",
        "    description=\"Ask a question and the chatbot will respond using a pre-defined QA dataset or Llama 2 if the answer is not in the dataset.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6Aq7xYmr0x"
      },
      "outputs": [],
      "source": [
        "# Launch the Gradio Interface\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4zxpuo5mtZV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9230f4b77fa4e8e840b1b0d010e1331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd0deb3861a34853ac6dee48c977190b",
              "IPY_MODEL_dbe99ed017b64b09a59612916a838e0c",
              "IPY_MODEL_55cc544138e04bb891ca23aa46a24f33"
            ],
            "layout": "IPY_MODEL_82f1f3675e404a17a7ca3421f8098b4e"
          }
        },
        "bd0deb3861a34853ac6dee48c977190b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337235a39fcb47a59032d42fb0735dd5",
            "placeholder": "​",
            "style": "IPY_MODEL_12dd966286594e1f9b250a023d326adc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dbe99ed017b64b09a59612916a838e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a1585cde5e440b78498782c1d470bf9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64eb7255a44f4fb28c9da034807678b8",
            "value": 2
          }
        },
        "55cc544138e04bb891ca23aa46a24f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d95e084c3a44967acd127821d7164fa",
            "placeholder": "​",
            "style": "IPY_MODEL_7357b5c1349240218bd04a3d9e99cb48",
            "value": " 2/2 [00:08&lt;00:00,  3.41s/it]"
          }
        },
        "82f1f3675e404a17a7ca3421f8098b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337235a39fcb47a59032d42fb0735dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12dd966286594e1f9b250a023d326adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a1585cde5e440b78498782c1d470bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64eb7255a44f4fb28c9da034807678b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d95e084c3a44967acd127821d7164fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7357b5c1349240218bd04a3d9e99cb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}